---
layout: about
title: about
permalink: /
subtitle: |
  Athens, Greece 
profile:
  align: right
  image: me.png
  image_circular: false # crops the image to make it circular
  more_info:

news: True # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

I like to do research in machine learning, NLP, and cognitive science.   
Currently applying to PhD positions.

You can email me at **nikitastheodorop@gmail.com**   
See other ways to connect below.     
You can find my CV [here <i class="fa-solid fa-arrow-up-right-from-square"></i>](assets/pdf/cv_nikitas_theodoropoulos.pdf).


##### Research Interests

Through my research, I want to understand fundamental aspects of intelligence, both in machines and humans, with an emphasis on language.   
Some of the topics I am interested in are:

- Computational models of human language acquisition and processing
- Human-like models, that are computation and sample efficient
- Multi-agent interaction and communication
- Embodied agents and language grounding
- Multilinguality and low-resource languages 
- Interpretability and the scientific study of language models
- Language emergence and evolution


##### Short CV

<!-- (https://www.ails.ece.ntua.gr/) ](https://www.mpi-sws.org/)-->

I graduated with a BSc & MSc in Electrical and Computer Engineering from the [National Technical University of Athens](https://www.ece.ntua.gr/en)  (NTUA). I completed my thesis at the Artificial Intelligence and Learning Systems Lab (AILS), supervised by Prof. [Giorgos Stamou](https://www.ece.ntua.gr/en/staff/174). In our research, we investigated language modeling with human-like data constraints of at most 100 million words, as part of the 2024 [BabyLM Challenge](https://babylm.github.io/). 

<!-- We trained GPT-Neo models on subsets of TinyStories, and generated synthetic data for training an encoder BERT-style model. Our results demonstrate that even with low amounts of data (25 million words) our GPT models can generate grammatical and coherent stories. -->

During my studies, I interned with the [Machine Teaching](https://machineteaching.mpi-sws.org/) group at MPI-SWS, where I was advised by Prof. [Adish Singla](https://machineteaching.mpi-sws.org/adishsingla.html). We worked on AI for programming education focusing on block-based visual programming (e.g., Scratch). Our work focused on modeling and predicting student behavior. We designed a benchmark for synthesizing a student's attempt at an unknown task, given the student's attempt at a known reference task. 

<!-- We proposed a neural, program-synthesizer based method, and a probabilistic context-free grammar (PCFG) symbolic method as baselines, and conduct a survey to evaluate human performance.  -->


In the past, I also worked with Prof. [Alexandros Potamianos](https://slp-ntua.github.io/potam/) at the [Speech and Language](https://slp-ntua.github.io/index.html) group at NTUA. Our research topic was learning brain-derived (fMRI) word representations and applying them to NLP tasks. 

<!-- A sample of this work can be found [here](). -->


##### More about me


<!-- I'm always looking for learning resources (for topics mentioned in my interests, and others), if you have a good resource to recommend, reach out! (In the meantime you can also check some recommended resources that I collected). -->
<!-- If there is any help/advice I can give, or you just want to chat about common interests, feel free to reach out. -->
In my free time, I enjoy indoor climbing ðŸ§—, playing the piano ðŸŽ¹, and reading ðŸ“š.   

I also like to learn new and exciting programming languages, and I am interested in open-source, decentralized, and self-hosted software.



